# description
# MPI based file operations
mpiFileUtils are MPI applications and must be launched within a compute allocation on a cluster using mpirun. The tools do not currently checkpoint, so one must be careful that an invocation of the tool has sufficient time to complete before it is killed.

# see also
mutil - https://github.com/pkolano/mutil/blob/master/patch/coreutils-8.22.patch (patches to coreutils)
dents
dstats
dirent
go-dirscan

# overview
dbcast - Broadcast a file to each compute node.
dbz2 - Compress and decompress a file with bz2.
dchmod - Change owner, group, and permissions on files.
dcmp - Compare contents between directories or files.
dcp - Copy files.
ddup - Find duplicate files.
dfind - Filter files.
dreln - Update symlinks to point to a new path.
drm - Remove files.
dstripe - Restripe files (Lustre).
dsync - Synchronize source and destination directories or files.
dtar - Create and extract tape archive files.
dwalk - List, sort, and profile files.

# Experimental UtilitiesExperimental utilities are under active development.
dgrep - Run grep on files in parallel.
dparallel - Perform commands in parallel.
dsh - List and remove files with interactive commands.
dfilemaker - Generate random files.

# usage
- run the tools within a job allocation. The sweet spot for most tools is 2-4 nodes. k
- can use more nodes for large datasets, so long as tools scale sufficiently well.
- launch the job using the MPI job launcher like mpirun or mpiexec.
- should use most CPU cores, though leave a few cores idle on each node for the file system client processes.
- Most tools do not checkpoint their progress. Be sure to request sufficient time in your allocation to allow the job to complete
- cannot pipe output of one tool to the input of another. However, the --input and --output file options are good approximations.
- cannot easily check the return codes of tools. Instead, inspect stdout and stderr output for errors.

# save output mfu
dwalk --output list.mfu /path/to/walk

# convert mfu to mft
dwalk --input list.mfu --text --output list.txt

# delete all files in the purge list
drm --input purgelist.mfu

# dcp
spack load /...
dori_srun_mpi 2
mpirun -n 2 dcp --bufsize 16MB -v  ~/DATA/tenG /tmp/accopeland

# dwalk sort -- By default, the list of files dwalk captures is not sorted.
A field name can be preceded with ‘-’ to sort by that field in reverse order.
A lexicographic sort is executed if more than one field is given.
To sort the list, one or more fields can be specified in a comma-delimited list:
name,user,group,uid,gid,atime,mtime,ctime,size

# dwalk sort -- print a list of files, sorted by file size, then by file name:
mpirun -np 128 dwalk –print –sort size,name /dir/to/walk

# dwalk histogram
Print the file distribution for specified histogram based on the size field from the top level directory.
mpirun -np 128 dwalk -v –print -d size:0,20,1G src/

# dwalk dori
 mpirun -np 2 dwalk  /clusterfs/jgi/groups/gentech/
numactl -C 4 mpirun -np 16 dwalk /clusterfs/jgi/groups/gentech/genome_analysis/
mpirun -np 16 numactl -C 0-15 dwalk /clusterfs/jgi/groups/gentech/genome_analysis/

# If your MPI library supports it, tools can run w/o mpirun as a single-task MPI job.
mpirun -np 128 dwalk /path/to/walk

# dwalk - output
# When walking large dir, write the list to an output file. you can read that list back w/out having to rewalkFS
dwalk --output list.mfu /path/to/walk
dwalk --input list.mfu

# output
# default file is binary but text is possible. text-based output is lossy, and can't be read back in to a tool.
# If you want both, save to binary format first, then read the binary file to convert it to text.:
dwalk --text --output list.txt /path/to/walk
dwalk --output list.mfu /path/to/walk
dwalk --input list.mfu --text --output list.txt

# dwalk sort -- sort option to order items in the list in various ways, e.g., to order the list by username, then by access time:
dwalk --input list.mfu --sort user,atime --output user_atime.mfu
# To order items from largest to smallest number of bytes:
dwalk --input list.mfu --sort '-size' --output big_to_small.mfu

dfind can be used to filter items with a string of find-like expressions, e.g., files owned by user1 that are bigger than 100GB:

dfind --input list.mfu --user user1 --size +100GB --output user1_over_100GB.mfu
dchmod is like chmod and chgrp in one, so one can change uid/gid/mode with a single command:

dchmod --group grp1 --mode g+rw /path/to/walk
drm is like "rm -rf" but in parallel:

drm /path/to/remove
dbcast provides an efficient way to broadcast a file to all compute nodes, e.g., upload a tar file of a dataset to an SSD local to each compute node:

dbcast /path/to/file.dat /ssd/file.dat
dsync is the recommended way to make a copy a large set of files:

dsync /path/src /path/dest
For large directory trees, the --batch-files option offers a type of checkpoint. It moves files in batches, and if interrupted, a restart picks up from the last completed batch.:

dsync --batch-files 100000 /path/src /path/dest
The tools can be composed in various ways using the --input and --output options. For example, the following sequence of commands executes a purge operation, which deletes any file that has not been accessed in the past 180 days.:

# walk directory to stat all files, record list in file
dwalk --output list.mfu /path/to/walk

# filter list to identify all regular files that were last accessed over 180 days ago
dfind --input list.mfu --type f --atime +180 --output purgelist.mfu
